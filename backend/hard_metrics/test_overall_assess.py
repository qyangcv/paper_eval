"""
æµ‹è¯• overall_assess æ¨¡å—çš„æ ¸å¿ƒåŠŸèƒ½
ä¸ä¾èµ–å¤–éƒ¨AIæ¨¡å‹ï¼Œåªæµ‹è¯•æ•°æ®å¤„ç†é€»è¾‘
"""

import os
import sys
import tempfile
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from pipeline.overall_assess import (
    extract_toc_and_chapters,
    generate_selection_prompt,
    parse_selected_chapters,
    generate_final_assessment_prompt
)

def create_test_markdown():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„markdownæ–‡ä»¶"""
    content = """
# æµ‹è¯•è®ºæ–‡æ ‡é¢˜

## ç›®å½•
1. ç»ªè®º
2. ç›¸å…³å·¥ä½œ  
3. ç ”ç©¶æ–¹æ³•
4. å®éªŒç»“æœ
5. ç»“è®º

## æ‘˜è¦
è¿™æ˜¯ä¸€ç¯‡æµ‹è¯•è®ºæ–‡çš„æ‘˜è¦ï¼ŒåŒ…å«ç ”ç©¶èƒŒæ™¯ã€æ–¹æ³•å’Œä¸»è¦è´¡çŒ®ã€‚

# ç»ªè®º
æœ¬ç« ä»‹ç»ç ”ç©¶èƒŒæ™¯å’Œæ„ä¹‰ã€‚

## ç ”ç©¶èƒŒæ™¯
æè¿°ç ”ç©¶èƒŒæ™¯çš„è¯¦ç»†å†…å®¹ã€‚

# ç›¸å…³å·¥ä½œ
å›é¡¾ç›¸å…³é¢†åŸŸçš„ç ”ç©¶å·¥ä½œã€‚

# ç ”ç©¶æ–¹æ³•
è¯¦ç»†ä»‹ç»æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ã€‚

## ç®—æ³•è®¾è®¡
æè¿°æ ¸å¿ƒç®—æ³•çš„è®¾è®¡æ€è·¯ã€‚

# å®éªŒç»“æœ
å±•ç¤ºå®éªŒç»“æœå’Œåˆ†æã€‚

# ç»“è®º
æ€»ç»“å…¨æ–‡çš„ä¸»è¦è´¡çŒ®å’Œæœªæ¥å·¥ä½œã€‚
"""
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as f:
        f.write(content)
        return f.name

def test_extract_toc_and_chapters():
    """æµ‹è¯•ç« èŠ‚æå–åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•ç« èŠ‚æå–åŠŸèƒ½...")
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_file = create_test_markdown()
    
    try:
        # æå–æ•°æ®
        data = extract_toc_and_chapters(test_file)
        
        # éªŒè¯ç»“æœ
        assert 'toc' in data, "ç¼ºå°‘ç›®å½•æ•°æ®"
        assert 'abstract' in data, "ç¼ºå°‘æ‘˜è¦æ•°æ®"
        assert 'chapters' in data, "ç¼ºå°‘ç« èŠ‚æ•°æ®"
        
        print(f"âœ… æå–åˆ°ç›®å½•: {len(data['toc'])} å­—ç¬¦")
        print(f"âœ… æå–åˆ°æ‘˜è¦: {len(data['abstract'])} å­—ç¬¦")
        print(f"âœ… æå–åˆ°ç« èŠ‚: {len(data['chapters'])} ä¸ª")
        print(f"   ç« èŠ‚æ ‡é¢˜: {list(data['chapters'].keys())}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç« èŠ‚æå–æµ‹è¯•å¤±è´¥: {e}")
        return False
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(test_file)

def test_generate_selection_prompt():
    """æµ‹è¯•é€‰æ‹©æç¤ºè¯ç”Ÿæˆ"""
    print("\nğŸ§ª æµ‹è¯•é€‰æ‹©æç¤ºè¯ç”Ÿæˆ...")
    
    try:
        toc = "1. ç»ªè®º\n2. ç›¸å…³å·¥ä½œ\n3. ç ”ç©¶æ–¹æ³•\n4. å®éªŒç»“æœ"
        abstract = "è¿™æ˜¯æµ‹è¯•æ‘˜è¦å†…å®¹"
        metric = "åˆ›æ–°æ€§"
        
        prompt = generate_selection_prompt(toc, abstract, metric)
        
        # éªŒè¯æç¤ºè¯åŒ…å«å¿…è¦ä¿¡æ¯
        assert metric in prompt, f"æç¤ºè¯ä¸­ç¼ºå°‘è¯„ä¼°ç»´åº¦: {metric}"
        assert toc in prompt, "æç¤ºè¯ä¸­ç¼ºå°‘ç›®å½•ä¿¡æ¯"
        assert abstract in prompt, "æç¤ºè¯ä¸­ç¼ºå°‘æ‘˜è¦ä¿¡æ¯"
        assert "JSON" in prompt, "æç¤ºè¯ä¸­ç¼ºå°‘è¾“å‡ºæ ¼å¼è¯´æ˜"
        
        print(f"âœ… æˆåŠŸç”Ÿæˆé€‰æ‹©æç¤ºè¯ï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦")
        return True
        
    except Exception as e:
        print(f"âŒ é€‰æ‹©æç¤ºè¯ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_parse_selected_chapters():
    """æµ‹è¯•ç« èŠ‚è§£æåŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•ç« èŠ‚è§£æåŠŸèƒ½...")
    
    try:
        # æµ‹è¯•æ­£å¸¸JSONå“åº”
        json_response = """
        ```json
        {
            "selected_chapters": ["ç»ªè®º", "ç ”ç©¶æ–¹æ³•", "å®éªŒç»“æœ"],
            "reasoning": "è¿™äº›ç« èŠ‚æœ€èƒ½ä½“ç°åˆ›æ–°æ€§"
        }
        ```
        """
        
        chapters = parse_selected_chapters(json_response)
        expected = ["ç»ªè®º", "ç ”ç©¶æ–¹æ³•", "å®éªŒç»“æœ"]
        
        assert chapters == expected, f"è§£æç»“æœä¸åŒ¹é…: {chapters} vs {expected}"
        print(f"âœ… JSONè§£ææˆåŠŸ: {chapters}")
        
        # æµ‹è¯•é™çº§å¤„ç†
        fallback_response = "æˆ‘é€‰æ‹©äº†'ç»ªè®ºç« 'å’Œ'æ–¹æ³•ç« 'è¿›è¡Œè¯„ä¼°"
        chapters = parse_selected_chapters(fallback_response)
        
        print(f"âœ… é™çº§è§£ææˆåŠŸ: {chapters}")
        return True
        
    except Exception as e:
        print(f"âŒ ç« èŠ‚è§£ææµ‹è¯•å¤±è´¥: {e}")
        return False

def test_generate_final_assessment_prompt():
    """æµ‹è¯•æœ€ç»ˆè¯„ä¼°æç¤ºè¯ç”Ÿæˆ"""
    print("\nğŸ§ª æµ‹è¯•æœ€ç»ˆè¯„ä¼°æç¤ºè¯ç”Ÿæˆ...")
    
    try:
        content = "è¿™æ˜¯é€‰ä¸­ç« èŠ‚çš„å†…å®¹ï¼ŒåŒ…å«è¯¦ç»†çš„æŠ€æœ¯æè¿°å’Œå®éªŒæ•°æ®ã€‚"
        metric = "å®éªŒå®Œæˆåº¦"
        
        prompt = generate_final_assessment_prompt(content, metric)
        
        # éªŒè¯æç¤ºè¯åŒ…å«å¿…è¦ä¿¡æ¯
        assert metric in prompt, f"æç¤ºè¯ä¸­ç¼ºå°‘è¯„ä¼°ç»´åº¦: {metric}"
        assert content in prompt, "æç¤ºè¯ä¸­ç¼ºå°‘ç« èŠ‚å†…å®¹"
        assert "summary" in prompt, "æç¤ºè¯ä¸­ç¼ºå°‘è¾“å‡ºæ ¼å¼è¯´æ˜"
        assert "strengths" in prompt, "æç¤ºè¯ä¸­ç¼ºå°‘ä¼˜åŠ¿å­—æ®µ"
        assert "weaknesses" in prompt, "æç¤ºè¯ä¸­ç¼ºå°‘ä¸è¶³å­—æ®µ"
        
        print(f"âœ… æˆåŠŸç”Ÿæˆæœ€ç»ˆè¯„ä¼°æç¤ºè¯ï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦")
        return True
        
    except Exception as e:
        print(f"âŒ æœ€ç»ˆè¯„ä¼°æç¤ºè¯ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 60)
    print("ğŸ“‹ å¼€å§‹æµ‹è¯• overall_assess æ¨¡å—æ ¸å¿ƒåŠŸèƒ½")
    print("=" * 60)
    
    tests = [
        test_extract_toc_and_chapters,
        test_generate_selection_prompt,
        test_parse_selected_chapters,
        test_generate_final_assessment_prompt
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼overall_assess æ¨¡å—æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸")
    else:
        print(f"âš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
    
    print("=" * 60)

if __name__ == "__main__":
    main() 